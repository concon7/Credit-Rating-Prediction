{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILTKLDDAhn6O"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import random, jit, grad, vmap\n",
        "from jax.example_libraries import stax, optimizers\n",
        "from jax.example_libraries.stax import Dense, Relu, Flatten, Softmax, LogSoftmax, Tanh\n",
        "import jax.scipy.stats.norm as norm\n",
        "from jax.scipy.stats import multivariate_normal\n",
        "from jax.nn import sigmoid, log_sigmoid\n",
        "from jax.nn import one_hot\n",
        "import itertools\n",
        "import torch.utils as trch\n",
        "from torch.utils import data\n",
        "\n",
        "from tqdm import trange\n",
        "from functools import partial\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pymc as pm\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as npo"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKmtDss-iHDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e723ccba-261c-47f6-fd67-e4043a8347ce"
      },
      "source": [
        "# Ground Truth\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_data/data.csv\", delimiter=\",\", header='infer', \n",
        "                 infer_datetime_format=True, parse_dates=True,\n",
        "                  dayfirst=True)\n",
        "\n",
        "headers = df.columns\n",
        "data = df[headers[5:58]].to_numpy().astype(\"float32\")  # financial ratios\n",
        "flag = df[headers[67:120]].to_numpy().astype(\"int32\")  # flag 1, if data==0 or NaN\n",
        "train_df = pd.DataFrame(df)  # make copy\n",
        "\n",
        "for fin_ratio_ in headers[67:120]:\n",
        "    train_df = train_df[train_df[fin_ratio_] > 0]  # Eliminate NaN & zeros\n",
        "data_train = train_df[headers[5:58]].to_numpy().astype(\"float32\")\n",
        "flag_train = train_df[headers[67:120]].to_numpy().astype(\"int32\")\n",
        "\n",
        "sortedFullRatings = ['AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-', 'BBB+', 'BBB', \n",
        "                     'BBB-', 'BB+', 'BB', 'BB-',\n",
        "                     'B+', 'B', 'B-', 'CCC+', 'CCC', 'CCC-', 'D']\n",
        "sortedMainRatings = ['AAA', 'AA', 'A', 'BBB', 'BB', 'B', 'CCC', 'D']\n",
        "lessMainRatings = ['As', 'BBB', 'BB', 'B', 'C/Ds']\n",
        "# lessMainRatings = ['I', 'S']       \n",
        "# adjust to have # labels equal to mapRatings unique elements\n",
        "mapRatings = npo.array([0, 0, 0, 1, 2, 3, 4, 4])\n",
        "Y_full = npo.zeros((train_df.shape[0], len(sortedFullRatings))).astype(\"int32\")\n",
        "Y_main = npo.zeros((train_df.shape[0], len(sortedMainRatings))).astype(\"int32\")\n",
        "Y_less = npo.zeros((train_df.shape[0], mapRatings.max() + 1)).astype(\"int32\")\n",
        "for i in range(Y_full.shape[0]):\n",
        "    Y_full[i, sortedFullRatings.index(train_df[\"Rating\"][train_df.index[i]])] = 1\n",
        "    Y_main[i, sortedMainRatings.index(train_df[\"RatingMain\"][train_df.index[i]])] = 1\n",
        "    Y_less[i, mapRatings[sortedMainRatings.index(train_df[\"RatingMain\"][train_df.index[i]])]] = 1\n",
        "\n",
        "Y_color = np.argmax(Y_less, axis=1)  # Use Y_less, Y_main, Y_full depending on how many labels\n",
        "\n",
        "data_train_norm = (data_train - data_train.mean(axis=0)) #/ data_train.std(axis=0)\n",
        "\n",
        "pca = PCA()  # Initialize with n_components=x parameter to only find the top eigenvectors\n",
        "z = pca.fit_transform(data_train_norm)\n",
        "# z = z / z.std(axis=0)   # normalise across companies for each reduced feature\n",
        "n_pcs = pca.components_.shape[0]\n",
        "\n",
        "print(\"1st component explains: {s}% variance\".format(s=pca.explained_variance_ratio_[0] * 100))\n",
        "threshold = np.where(np.cumsum(pca.explained_variance_ratio_) >= .95)[0][0] + 1\n",
        "print(\"95% variance explained by {s} components\".format(s=threshold))\n",
        "\n",
        "X = data_train_norm\n",
        "Y = Y_less\n",
        "d = X.shape[1]                          # number of features\n",
        "N = Y_less.shape[0]                     # number observations\n",
        "k = Y_color.max() + 1                   # number of classifications"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st component explains: 62.83540725708008% variance\n",
            "95% variance explained by 3 components\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    fmt = '.2f' if normalize else '.0f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "K1oytSBaG7aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbe0nIx3DbYh"
      },
      "source": [
        "randomOrder = npr.permutation(N)  # random sampling of observations\n",
        "s = int(8 * N / 10)  # split training vs. testing 80:20\n",
        "x_train = X[randomOrder[0:s], :]\n",
        "y_train = np.array(Y[randomOrder[0:s], :])\n",
        "x_test = X[randomOrder[s:N], :]\n",
        "y_test = np.array(Y[randomOrder[s:N], :])   \n",
        "k = Y_color.max()+1\n",
        "y_train_flat = Y_color[randomOrder[0:s]]\n",
        "y_test_flat = Y_color[randomOrder[s:N]] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEYWQI32HfKA"
      },
      "source": [
        "# Architecture\n",
        "def NN(k):\n",
        "    init, apply = stax.serial(Dense(16),   # hidden Layers\n",
        "                              Tanh,\n",
        "                              Dense(k),   #final dense layers =  num_classes\n",
        "                              Softmax)\n",
        "    return init, apply\n",
        "\n",
        "class NNclassifier:\n",
        "    # Initialize the class\n",
        "    def __init__(self, d, k, rng_key=random.PRNGKey(0)):\n",
        "        # MLP init and apply functions\n",
        "        self.net_init, self.net_apply = NN(k)\n",
        "        _, params = self.net_init(rng_key, (-1, d)) \n",
        "\n",
        "        # Optimizer initialization and update functions\n",
        "        lr = optimizers.exponential_decay(1e-3, decay_steps=1000, decay_rate=0.999)\n",
        "        self.opt_init, \\\n",
        "        self.opt_update, \\\n",
        "        self.get_params = optimizers.adam(lr)\n",
        "        self.opt_state = self.opt_init(params)\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "\n",
        "        # Logger\n",
        "        self.itercount = itertools.count()\n",
        "        self.loss_log = []\n",
        "\n",
        "    def loss(self, params, batch):\n",
        "        data, labels = batch\n",
        "        outputs = self.net_apply(params, data)\n",
        "        loss = -labels*np.log(outputs)\n",
        "        return np.mean(loss)\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def step(self, i, opt_state, batch):\n",
        "        params = self.get_params(opt_state)\n",
        "        gradients = grad(self.loss)(params, batch)\n",
        "        return self.opt_update(i, gradients, opt_state)\n",
        "\n",
        "    def train(self, dataset, nIter = 10):\n",
        "        data = iter(dataset)\n",
        "        pbar = trange(nIter)\n",
        "        # Main training loop\n",
        "        for it in pbar:\n",
        "            # Run one gradient descent update\n",
        "            batch = next(data)\n",
        "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch)  \n",
        "            if it % 50 == 0:\n",
        "                # Logger\n",
        "                params = self.get_params(self.opt_state)\n",
        "                loss = self.loss(params, batch)\n",
        "                self.loss_log.append(loss)\n",
        "                pbar.set_postfix({'Loss': loss})\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def predict(self, params, inputs):\n",
        "        outputs = self.net_apply(params, inputs)\n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZjOsZeNxBVt"
      },
      "source": [
        "class DataGenerator(trch.data.Dataset):\n",
        "  \n",
        "    def __init__(self, images, labels, \n",
        "                 batch_size=128, \n",
        "                 rng_key=random.PRNGKey(1234)):\n",
        "        'Initialization'\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.N = labels.shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.key = rng_key\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def __data_generation(self, key, images, labels):\n",
        "        'Generates data containing batch_size samples'\n",
        "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
        "        images = images[idx,...]\n",
        "        labels = labels[idx,...]\n",
        "        return images, labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        self.key, subkey = random.split(self.key)\n",
        "        images, labels = self.__data_generation(self.key, self.images, self.labels)\n",
        "        return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTZR3EOts-bM"
      },
      "source": [
        "# Create data iteraror\n",
        "num_classes = k\n",
        "dim = x_train.shape[1]\n",
        "train_dataset = DataGenerator(x_train, y_train, batch_size=64)\n",
        "print('Input Data shape:', x_train.shape, '\\nOutput Label shape', y_train.shape)\n",
        "\n",
        "# Initialize model\n",
        "model = NNclassifier(dim, num_classes)\n",
        "\n",
        "# Train model\n",
        "model.train(train_dataset, nIter=2000)\n",
        "opt_params = model.get_params(model.opt_state)\n",
        "pred_train = np.argmax(model.predict(opt_params, x_train),1)\n",
        "\n",
        "weights = model.get_params(model.opt_state)\n",
        "print(\"Input weights to Hidden Layer:\\n\",weights[0][0].shape)\n",
        "print(\"Activation(Hidden Layer (16) weights):\\n\",weights[0][1].shape)\n",
        "print(\"Tanh:\\n\",weights[1])\n",
        "print(\"Hidden Layer to Output Layer weights:\\n\", weights[2][0].shape)\n",
        "print(\"Activation(Output Layer (2) weights):\\n\", weights[2][1].shape)\n",
        "print(\"Softmax:\\n\",weights[3])\n",
        "\n",
        "# Plot loss\n",
        "plt.figure()\n",
        "plt.plot(model.loss_log, lw=2)\n",
        "plt.title('Training Accuracy {0:.2f}'.format(np.mean(y_train_flat==pred_train)))\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Iter #')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Create data iteraror \n",
        "test_dataset = DataGenerator(x_test, y_test, batch_size=64)  # run only 2 dimensions to test\n",
        "test_data = iter(test_dataset)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "outputs = model.predict(opt_params, x_test)\n",
        "pred_class = np.argmax(outputs, 1)\n",
        "cm = confusion_matrix(y_test_flat, pred_class)\n",
        "plot_confusion_matrix(cm, lessMainRatings, normalize=False, \n",
        "                      title='NN Classifier Confusion matrix \\n Testing Accuracy {0:.2f}'.format(np.mean(y_test_flat==pred_class)), cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "print(\"Test Accuracy: \", np.mean(y_test_flat==pred_class))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}